defaults:
  - _self_

hydra:
  run:
    # dir: ${exp_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}
    dir: ${exp_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${exp_dir}/${hydra.job.name}  
  job:
    env_set:
      CUDA_VISIBLE_DEVICES: ${hydra.job.num}
    config:
      override_dirname:
        exclude_keys:
          - exp_dir
          - seed
          - wandb
          - model_path
          - exp_name
          - loop.training_steps
          - loop.finetune_steps
    chdir: true


prompt: "A mystic river flowing through a mountain." # the prompt we are fine-tuning for
exp_dir: ${oc.env:SCRATCH}/${oc.env:USER}/fdc_neurips2025/stable_diffusion_ent_kl/${now} # this is the directory where the experiment will be saved
cuda_device: 3
store_model: true
seed: 0

model:
  _target_: maxentdiff.models.StableDiffusion
  device: cuda
  guidance_scale: 8
  dtype: float32

diversity_evaluator:
  _target_: maxentdiff.sd_utils.CLIPInceptionDiversityEval
  model: ViT-B-32
  pretrained: laion2b_s34b_b79k

image_sampler:
  _target_: maxentdiff.sd_utils.ImageSampler
  num_images: [4,5]
  steps: 30
  guidance_scale: 8
  seed: 0
  prompt: ${prompt}

wandb:
  project: neurips-fdc-stable-diffusion
  tags: [SD-v1]
  name: ${prompt}/${hydra:job.override_dirname}/stable-diffusion/seed=${seed}-alpha=${trainer.alpha_div}-${now:%Y-%m-%d}
  entity: mvlast
  resume: auto
  

trainer:
  _target_: maxentdiff.trainers.max_ent_KL_reg.MaxEntKLRegTrainer
  alpha_div: 0.1 # alpha for divergence loss
  lr: 3.e-7
  traj_samples_per_stage: 5
  data_shape: [16384] # 4x64x64
  batch_size: 8
  traj_len: 30
  lmbda: 0.1 # 0.01
  finetune_steps: 10
  clip_grad_norm: 1.0
  sample_jumps: true # sample jumps in the diffusion process
  epsilon: 5.e-4

loop:
  md_steps: 20
  save_every: 5
  training_steps: 400
  log_freq: 5


  